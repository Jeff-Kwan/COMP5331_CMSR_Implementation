latent_factor_model: BPR
embedding_size: 64
mlp_hidden_size: [128]
k: 10
map_batch_size: 1024
train_epochs: ["SOURCE:100","TARGET:100","BOTH:50","TARGET:100"]
