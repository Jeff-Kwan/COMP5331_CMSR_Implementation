embedding_size: 64
lambda: 0.25
margin: 1
mlp_hidden_size: [128]
train_epochs: ["SOURCE:200","TARGET:200","OVERLAP:200"]
overlap_batch_size: 100
